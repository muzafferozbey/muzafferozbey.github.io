
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://muzafferozbey.github.io/theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark), (prefers-color-scheme: no-preference)"
    href="https://muzafferozbey.github.io/theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark), (prefers-color-scheme: no-preference)"
          href="https://muzafferozbey.github.io/theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light)"
          href="https://muzafferozbey.github.io/theme/pygments/monokai.min.css">


  <link rel="stylesheet" type="text/css" href="https://muzafferozbey.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://muzafferozbey.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://muzafferozbey.github.io/theme/font-awesome/css/solid.css">







    <meta name="author" content="Muzaffer Özbey" />
    <meta name="description" content="" />
<meta property="og:site_name" content="Muzaffer Özbey Personal Website"/>
<meta property="og:type" content="blog"/>
<meta property="og:title" content="Muzaffer Özbey Personal Website"/>
<meta property="og:description" content=""/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://muzafferozbey.github.io"/>
<meta property="og:image" content="/images/profil.png">



  <title>Muzaffer Özbey Personal Website &ndash; Projects</title>

</head>
<body >
  <aside>
    <div>
      <a href="https://muzafferozbey.github.io">
        <img src="/images/profil.png" alt="Muzaffer Özbey" title="Muzaffer Özbey">
      </a>

      <h1>
        <a href="https://muzafferozbey.github.io">Muzaffer Özbey</a>
      </h1>

<p> Graduate Student at Bilkent University, EE Department</p>

      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="https://muzafferozbey.github.io/pages/awards.html#awards">
                  Awards & Honors
                </a>
              </li>
              <li>
                <a target="_self"
                   href="https://muzafferozbey.github.io/pages/education.html#education">
                  Education
                </a>
              </li>
              <li>
                <a target="_self"
                   href="https://muzafferozbey.github.io/pages/projects.html#projects">
                  Projects
                </a>
              </li>
              <li>
                <a target="_self"
                   href="https://muzafferozbey.github.io/pages/publications.html#publications">
                  Publications
                </a>
              </li>

            <li>
              <a target="_self" href="http://www.icon.bilkent.edu.tr/index.html" >IconLab</a>
            </li>
            <li>
              <a target="_self" href="mailto:muzafferozbey94@gmail.com" >e-mail</a>
            </li>
        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-linkedin" href="https://www.linkedin.com/in/muzaffer-özbey-870745229/" target="_blank">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
          <li>
            <a  class="sc-scholar" href="https://scholar.google.com.tr/citations?user=VaoGQnMAAAAJ&hl=en" target="_blank">
              <i class="fab fa-scholar"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>


<article class="single">
  <header>
    
    <h1 id="projects">Projects</h1>
  </header>
  <div>
<p>Abstract of publication projects</p>
<hr>
<h2><strong>MS Project:</strong> <em>'Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery'</em></h2>
<p><img src="/images/provo.png" alt="drawing" width="1000"/></p>
<p>Magnetic resonance imaging (MRI) offers the flexibility to image a given anatomic volume under a multitude of tissue contrasts. Yet, scan time considerations put stringent limits on the quality and diversity of MRI data. The gold-standard approach to alleviate this limitation is to recover high-quality images from data undersampled across various dimensions, most commonly the Fourier domain or contrast sets. A primary distinction among recovery methods is whether the anatomy is processed per volume or per cross-section. Volumetric models offer enhanced capture of global contextual information, but they can suffer from suboptimal learning due to elevated model complexity. Cross-sectional models with lower complexity offer improved learning behavior, yet they ignore contextual information across the longitudinal dimension of the volume. Here, we introduce a novel progressive volumetrization strategy for generative models (ProvoGAN) that serially decomposes complex volumetric image recovery tasks into successive cross-sectional mappings task-optimally ordered across individual rectilinear dimensions. ProvoGAN effectively captures global context and recovers fine-structural details across all dimensions, while maintaining low model complexity and improved learning behaviour. Comprehensive demonstrations on mainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields superior performance to state-of-the-art volumetric and cross-sectional models.</p>
<hr>
<h2><strong>MS Project:</strong> <em>'A Transfer-Learning Approach for Accelerated MRI Using Deep Neural Networks'</em></h2>
<p><img alt="" src="/images/transfer.png"></p>
<p>Neural networks have received recent interest for reconstruction of undersampled MR acquisitions. Ideally, network performance should be optimized by drawing the training and testing data from the same domain. In practice, however, large datasets comprising hundreds of subjects scanned under a common protocol are rare. The goal of this study is to introduce a transfer-learning approach to address the problem of data scarcity in training deep networks for accelerated MRI.</p>
<hr>
<h2><strong>MS Project:</strong> <em>'Semi-Supervised Learning of Mutually Accelerated MRI Synthesis without Fully-Sampled Ground Truths'</em></h2>
<p><img alt="" src="/images/ssgan.png"></p>
<p>Learning-based synthetic multi-contrast MRI commonly involves deep models trained using high-quality images of source and target contrasts, regardless of whether source and target domain samples are paired or unpaired. This results in undesirable reliance on fullysampled acquisitions of all MRI contrasts, which might prove impractical due to limitations on scan costs and time. Here, we propose a novel semi-supervised deep generative model that instead learns to recover high-quality target images directly from accelerated acquisitions of source and target contrasts. To achieve this, the proposed model introduces novel multi-coil tensor losses in image, k-space and adversarial domains. These selective losses are based only on acquired k-space samples, and randomized sampling masks are used across subjects to capture relationships among acquired and non-acquired k-space regions. Comprehensive experiments on multi-contrast neuroimaging datasets demonstrate that our semi-supervised approach yields equivalent performance to gold-standard fully-supervised models, while outperforming a cascaded approach that learns to synthesize based on reconstructions of undersampled data. Therefore, the proposed approach holds great promise to improve the feasibility and utility of accelerated MRI acquisitions mutually undersampled across both contrast sets and k-space</p>  </div>
</article>

    <footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://muzafferozbey.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="dark"
          type="text/javascript">
  </script>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Muzaffer Özbey Personal Website ",
  "url" : "https://muzafferozbey.github.io",
  "image": "/images/profil.png",
  "description": ""
}
</script>


</body>
</html>